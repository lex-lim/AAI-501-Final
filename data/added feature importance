import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the CSV file
file_path = ('/Users/posolga/Desktop/vgchartz-2024.csv')
df = pd.read_csv(file_path)

# Display the dataframe
print(df.head())

# Analyze the data structure 
print(df.shape)
print(df.info())
print(df.describe().T)
print(df.columns)

# If 'release_date' column is missing, we skip handling it
if 'release_date' in df.columns:
    # Handle missing values and data preprocessing before reverting one-hot encoding
    # Converting release date column 
    df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')
    df['release_year'] = df['release_date'].dt.year
    df['release_month'] = df['release_date'].dt.month

    # Drop the original release_date column as it's now converted
    df.drop('release_date', axis=1, inplace=True)

# Fill missing values for numeric columns only
df.fillna(df.mean(numeric_only=True), inplace=True)

# Drop additional unnecessary columns
columns_to_drop = ['img', 'title', 'last_update']  # Add more columns to drop if necessary
df.drop(columns=columns_to_drop, errors='ignore', inplace=True)

# Display the updated DataFrame
print(df.head())
print(df.columns)

# Initialize the LabelEncoder for each categorical column
le_console = LabelEncoder()
le_genre = LabelEncoder()
le_publisher = LabelEncoder()
le_developer = LabelEncoder()

# Apply label encoding to each column
df['console'] = le_console.fit_transform(df['console'])
df['genre'] = le_genre.fit_transform(df['genre'])
df['publisher'] = le_publisher.fit_transform(df['publisher'])
df['developer'] = le_developer.fit_transform(df['developer'])

# Display data types to ensure all features are numeric
print(df.dtypes)


# Splitting the data into training and testing sets
X = df.drop(['total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales'], axis=1)
y = df['total_sales']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Naive Regression Model: Predict the mean of total_sales
mean_total_sales = y_train.mean()
y_naive_pred = [mean_total_sales] * len(y_test)

# Calculate MAE and MSE for the naive regression model
naive_mae = mean_absolute_error(y_test, y_naive_pred)
naive_mse = mean_squared_error(y_test, y_naive_pred)

print(f'Naive Regression MAE: {naive_mae:.2f}')
print(f'Naive Regression MSE: {naive_mse:.2f}')


# Building linear regression predictor model
# Features and target variable
X = df.drop(['total_sales', 'na_sales', 'jp_sales'], axis=1)  # Drop columns that won't be used as features
y = df['total_sales']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Linear Regression MSE: {mse: .2f}')
print(f'Mean Absolute Error: {mae:.2f}')
print(f'Linear Regression R-squared: {r2: .2f}')



# Feature importance for Linear Regression
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
print("Feature importance for Linear Regression:")
print(coefficients)


# Use an existing encoded value for the publisher and developer
example_publisher = df['publisher'].iloc[0]
example_developer = df['developer'].iloc[0]

new_game = {
    'console': le_console.transform(['PS4'])[0],
    'genre': le_genre.transform(['Action'])[0],
    'publisher': example_publisher,
    'developer': example_developer,
    'critic_score': 9.0,
    'total_sales': 0, # Add dummy value for total_sales since it's not used as a feature
    'na_sales': 0,    # Add dummy value for na_sales
    'jp_sales': 0,    # Add dummy value for jp_sales
    'pal_sales': 0,   # Add dummy value for pal_sales
    'other_sales': 0, # Add dummy value for other_sales
    'release_year': 2018,
    'release_month': 5
}

# Create DataFrame with the same columns as the training data
new_game_df = pd.DataFrame([new_game])

# Make sure columns match those used during training
new_game_df = new_game_df[X.columns]

# Make prediction
predicted_sales = model.predict(new_game_df)
print(f'Predicted Total Sales: {predicted_sales[0]}')

# Create classification via decision tree
# Define thresholds for impact of title
tentpole_threshold = df['total_sales'].quantile(0.95)  # Top 25% sales
moderate_threshold = df['total_sales'].quantile(0.25)

# Create a new column for sales impact
def categorize_sales_impact(sales):
    if sales >= tentpole_threshold:
        return 'tentpole impact'
    elif sales >= moderate_threshold:
        return 'moderate impact'
    else:
        return 'minor impact'

df['sales_impact'] = df['total_sales'].apply(categorize_sales_impact)
print(df['sales_impact'].value_counts())

# --------------------------
# Building decision tree classifier model
# Features and target variable
X = df.drop(['total_sales', 'na_sales', 'jp_sales', 'sales_impact'], axis=1)  # Drop columns that won't be used as features
y = df['sales_impact']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:')
print(report)
print('Confusion Matrix:')
print(conf_matrix)


# Feature importance for Decision Tree
importances = model.feature_importances_
features = X.columns
feature_importance_df = pd.DataFrame(importances, index=features, columns=['Importance']).sort_values(by='Importance', ascending=False)
print("Feature importance for Decision Tree:")
print(feature_importance_df)

import matplotlib.pyplot as plt

# Data for visualization
features = ['console', 'genre', 'publisher', 'developer', 'critic_score', 'pal_sales', 'other_sales', 'release_year', 'release_month']
importance_dt = [0.103585, 0.029771, 0.044628, 0.050212, 0.011644, 0.049815, 0.622973, 0.056585, 0.030787]
coefficients_lr = [0.000432, 0.000221, -0.000005, 0.000001, 0.024891, 1.388233, 2.857765, -0.002722, 0.002099]

# Plot Decision Tree Feature Importance
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.barh(features, importance_dt, color='blue')
plt.xlabel('Importance')
plt.title('Decision Tree Feature Importance')
plt.gca().invert_yaxis()

# Plot Linear Regression Coefficients
plt.subplot(1, 2, 2)
plt.barh(features, coefficients_lr, color='green')
plt.xlabel('Coefficient')
plt.title('Linear Regression Feature Coefficients')
plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()
